from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, length, desc, split
from delta import configure_spark_with_delta_pip  # â¬…ï¸ Delta Lake

# --------------------------------------------------------------------
# CONFIGURAÃ‡Ã•ES GERAIS
# --------------------------------------------------------------------
MINIO_ENDPOINT = "http://minio:9000"
MINIO_ACCESS_KEY = "cursolab"
MINIO_SECRET_KEY = "cursolab"

# Trusted/Silver e Refined/Gold em Delta Lake
TRUSTED_PATH = "s3a://trusted/movies/"     # â¬…ï¸ Delta
REFINED_PATH = "s3a://refined/movies/"     # â¬…ï¸ Delta (destino das views)

# --------------------------------------------------------------------
# FUNÃ‡ÃƒO DE PROCESSAMENTO (DELTA IN/OUT)
# --------------------------------------------------------------------
def process_trusted_to_refined():
    # SparkSession com Delta + S3A (MinIO)
    builder = (
        SparkSession.builder.appName("TrustedToRefinedAirflow_Delta")
        # JARs locais do Hadoop AWS + SDK (necessÃ¡rios para s3a://)
        .config(
            "spark.jars",
            "/opt/spark/jars/hadoop-aws-3.3.4.jar,"
            "/opt/spark/jars/aws-java-sdk-bundle-1.12.262.jar"
        )
        # MinIO/S3A
        .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT)
        .config("spark.hadoop.fs.s3a.access.key", MINIO_ACCESS_KEY)
        .config("spark.hadoop.fs.s3a.secret.key", MINIO_SECRET_KEY)
        .config("spark.hadoop.fs.s3a.path.style.access", "true")
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        # Delta Lake (extensÃ£o + catÃ¡logo)
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    )
    spark = configure_spark_with_delta_pip(builder).getOrCreate()

    print("Lendo dados da camada Trusted (Delta Lake)...")
    # â¬‡ï¸ Leia a Trusted como Delta (NÃƒO parquet)
    df = spark.read.format("delta").load(TRUSTED_PATH)

    # ----------------------------------------------------------------
    # VisÃ£o 1 â€” Contagem de tÃ­tulos por tipo (Filme ou SÃ©rie)
    # ----------------------------------------------------------------
    v1 = df.groupBy("type").agg(count("*").alias("total"))
    (
        v1.write.format("delta")
        .mode("overwrite")                    # sobrescreve a visÃ£o mantendo time travel
        .option("overwriteSchema", "true")    # segura se o schema evoluir
        .save(f"{REFINED_PATH}v1_titles_by_type")
    )

    # ----------------------------------------------------------------
    # VisÃ£o 2 â€” Top 10 paÃ­ses com mais produÃ§Ãµes
    # ----------------------------------------------------------------
    v2 = (
        df.groupBy("country")
          .agg(count("*").alias("total"))
          .orderBy(desc("total"))
          .limit(10)
    )
    (
        v2.write.format("delta")
        .mode("overwrite").option("overwriteSchema", "true")
        .save(f"{REFINED_PATH}v2_top10_countries")
    )

    # ----------------------------------------------------------------
    # VisÃ£o 3 â€” DuraÃ§Ã£o mÃ©dia por categoria (Movie vs TV Show)
    # ----------------------------------------------------------------
    df_duration = df.withColumn("duration_num", split(col("duration"), " ").getItem(0).cast("int"))
    v3 = df_duration.groupBy("type").agg(avg("duration_num").alias("avg_duration"))
    (
        v3.write.format("delta")
        .mode("overwrite").option("overwriteSchema", "true")
        .save(f"{REFINED_PATH}v3_avg_duration")
    )

    # ----------------------------------------------------------------
    # VisÃ£o 4 â€” Quantidade de tÃ­tulos por ano de lanÃ§amento
    # ----------------------------------------------------------------
    v4 = (
        df.filter(col("release_year").isNotNull())
          .groupBy("release_year")
          .agg(count("*").alias("total"))
          .orderBy("release_year")
    )
    (
        v4.write.format("delta")
        .mode("overwrite").option("overwriteSchema", "true")
        .save(f"{REFINED_PATH}v4_titles_by_year")
    )

    # ----------------------------------------------------------------
    # VisÃ£o 5 â€” TÃ­tulos com descriÃ§Ãµes mais longas
    # ----------------------------------------------------------------
    v5 = (
        df.withColumn("desc_length", length(col("description")))
          .select("title", "type", "desc_length")
          .orderBy(desc("desc_length"))
          .limit(10)
    )
    (
        v5.write.format("delta")
        .mode("overwrite").option("overwriteSchema", "true")
        .save(f"{REFINED_PATH}v5_longest_descriptions")
    )

    print("âœ… Todas as visÃµes foram processadas e salvas em Delta na camada Refined (Gold).")
    spark.stop()

# --------------------------------------------------------------------
# DAG AIRFLOW
# --------------------------------------------------------------------
default_args = {
    "owner": "Fia",
    "start_date": datetime(2025, 10, 4),
    "retries": 1,
    "retry_delay": timedelta(minutes=2),
}

with DAG(
    dag_id="trusted_to_refined_pyspark_delta",
    default_args=default_args,
    schedule_interval="*/10 * * * *",  # a cada 10 minutos (ajustei o cron)
    catchup=False,
    tags=["pyspark", "airflow", "refined", "delta", "lakehouse"],
) as dag:

    start = PythonOperator(
        task_id="start",
        python_callable=lambda: print("Iniciando processamento Trusted â†’ Refined (Delta)"),
    )

    process = PythonOperator(
        task_id="process_trusted_to_refined",
        python_callable=process_trusted_to_refined,
    )

    end = PythonOperator(
        task_id="end",
        python_callable=lambda: print("ğŸ Pipeline Trusted â†’ Refined finalizado com sucesso"),
    )

    start >> process >> end
